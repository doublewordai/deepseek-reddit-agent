{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit GenAI Trend Analysis with ReAct Agent Framework\n",
    "\n",
    "Author: Amanda Milberg, Principal Solutions Engineer @ TitanML\n",
    "\n",
    "üéØ **Main Purpose**:\n",
    "- Analyzes r/technology subreddit posts to identify and summarize GenAI-related content\n",
    "- Generates professional summaries of AI trends and developments to send to downstream users who want to stay up to date on the latest trends\n",
    "\n",
    "üîë **Key Components**:\n",
    "1. Reddit API Integration to scrape relevant posts in a given subreddit (e.g. r/technology)\n",
    "2. LLM-powered analysis to:\n",
    "   - Determine GenAI relevance based on the thread title\n",
    "   - Summarize key themes and content for each article\n",
    "   - Generate trend analysis summary reports for all the GenAI related articles \n",
    "\n",
    "üìä **Process Flow**:\n",
    "1. Fetches hot posts from r/technology \n",
    "2. Filters for GenAI-related content\n",
    "3. Extracts and summarizes article content\n",
    "4. Creates comprehensive trend analysis\n",
    "5. Generates formatted report with sources ready to email to downstream users \n",
    "\n",
    "üõ†Ô∏è **Technologies Used**:\n",
    "- PRAW (Reddit API)\n",
    "- OpenAI API/Self-hosted LLM\n",
    "- BeautifulSoup for web scraping\n",
    "- Markdown for report formatting\n",
    "- ReAct agent framework\n",
    "\n",
    "_Note: Requires Reddit API credentials and access to a LLM to function._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use an Agent Framework?\n",
    "\n",
    "- Implements the ReAct (Reasoning + Acting) paradigm for more transparent and controlled AI behavior\n",
    "- Provides explicit thinking and action steps for complex tasks\n",
    "- Enables better debugging and monitoring of the AI's decision process\n",
    "\n",
    "üß† **ReAct Framework Benefits**:\n",
    "1. **Reasoning Transparency**\n",
    "   - Agent explicitly shows its thinking process before actions\n",
    "   - Helps track decision-making logic\n",
    "   - Makes debugging easier\n",
    "\n",
    "2. **Structured Actions**\n",
    "   - Clear separation between thinking and execution\n",
    "   - Each action has defined inputs and outputs\n",
    "   - Better error handling and recovery\n",
    "\n",
    "3. **Process Monitoring**\n",
    "   - Logs each step of the analysis pipeline\n",
    "   - Tracks success/failure of individual components\n",
    "   - Maintains history of decisions and actions\n",
    "\n",
    "_The agent framework transforms what could be a simple script into a more robust, observable, and maintainable system for AI analysis. The agent approach provides better structure, transparency, and reliability for complex AI tasks compared to a simple main function._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Self-Host?\n",
    "\n",
    "üåü **Key Benefits of Self-Hosting** \n",
    "\n",
    "1. **Cost-Effective Performance**\n",
    "   - Reduced operational costs for high-volume processing\n",
    "   - No ongoing API fees or usage limits\n",
    "\n",
    "2. **Privacy & Data Control** \n",
    "   - Complete control over data processing and storage\n",
    "   - No data sharing with external providers\n",
    "   - Compliance with internal security policies\n",
    "   - Ability to air-gap for sensitive applications & sensitive data \n",
    "\n",
    "3. **Deployment Flexibility**\n",
    "   - Run locally on your own infrastructure\n",
    "   - Scale resources based on actual needs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Deep Seek?\n",
    "\n",
    "1. **Specialized Reasoning Capabilities**\n",
    "   - Optimized for logical reasoning and analysis tasks\n",
    "   - Efficient chain-of-thought processing\n",
    "   - Ideal for structured analytical workflows\n",
    "2. **Open Source Technology + Self-Hosting Stack = üòç**  \n",
    "   - Deepseek broke the internet \n",
    "   - Firm believer in owning your AI stack \n",
    "   - Smaller / specalized models for a given application  \n",
    "\n",
    "_Note: In this demo we are running a self-hosted [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) deployed on 4xL4 GPUs using the [TitanML's Takeoff Stack](https://docs.titanml.co/). If you want to try this on your own you can pull this repository and swap in an OpenAI model. The code uses OpenAI compatiable endpoints so any model should be able to be swapped in. If you have any questions please reach out to: amanda.milberg@titanml.co_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions in AI Agent Architecture (or the \"Doing\")\n",
    "\n",
    "üîß **Service Functions**\n",
    "Functions that handle specific, specialized tasks like:\n",
    "- API interactions (init_reddit, init_llm)\n",
    "- Web scraping (extract_article_content)\n",
    "- Data parsing & formatting (parse_llm_response)\n",
    "- LLM analysis (analyze_genai_relevance, summarize_content, create_email_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "import re\n",
    "import requests\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "\n",
    "def init_reddit(client_id: str, client_secret: str, user_agent: str) -> praw.Reddit:\n",
    "    \"\"\"Initialize Reddit API client\"\"\"\n",
    "    return praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "\n",
    "def init_llm(api_key: str) -> OpenAI:\n",
    "    ## For practice at home, you can sub the self-hosted LLM for openAI LLM\n",
    "    \"\"\"Initialize OpenAI LLM Note: Need access to OpenAI Key\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    client = OpenAI(temperature=0.7)\n",
    "    \"\"\"\n",
    "    ## In our demo we will use a self-hosted LLM \n",
    "    client = OpenAI(\n",
    "    base_url=\"http://rag-demo:3003/v1\",\n",
    "    api_key=\"not needed\"\n",
    "    )\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "def extract_article_content(url: str) -> str:\n",
    "    \"\"\"Extract main content from article URL with proper headers\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return ' '.join(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def analyze_genai_relevance(llm: OpenAI, title: str) -> dict:\n",
    "    \"\"\"Analyze if title is GenAI-related using LLM\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Based on the title \n",
    "    of the article provide a suggestion if this content relates to Generative AI:\n",
    "    \n",
    "    Return JSON:\n",
    "        {{\n",
    "            \"is_genai_related\": true/false,\n",
    "            \"relevance_type\": \"direct/indirect/none\",\n",
    "        }}\"\"\"    \n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model = \"internvl\", ##switch to OpenAI model (e.g. gpt-4) for OpenAI implementation \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": title}\n",
    "            ],\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "        \n",
    "        # Extract the response content\n",
    "        response_dict = parse_llm_response(response.choices[0].text)\n",
    "        return response_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in GenAI relevance: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "def parse_llm_response(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse LLM response to separate thinking process and JSON response from \n",
    "    analyze_genai_relevance()\n",
    "    \"\"\"\n",
    "    # Pattern for think tags\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    \n",
    "    # Pattern for JSON (anything between triple backticks and json)\n",
    "    json_pattern = r'```json\\n(.*?)```'\n",
    "    \n",
    "    # Extract thinking process\n",
    "    thinking = re.search(think_pattern, response_text, re.DOTALL)\n",
    "    thinking = thinking.group(1).strip() if thinking else \"\"\n",
    "    \n",
    "    # Extract JSON response\n",
    "    json_match = re.search(json_pattern, response_text, re.DOTALL)\n",
    "    json_str = json_match.group(1).strip() if json_match else \"{}\"\n",
    "    json_data = json.loads(json_str)\n",
    "    \n",
    "    return {\n",
    "        \"thinking\": thinking,\n",
    "        \"response\": json_data\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_content(llm: OpenAI, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarize input text using the chat completions model directly\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Given a piece of text, analyze its content and provide a concise summary.\n",
    "    Focus on extracting key information and main ideas.\n",
    "    If the text contains technical terms, explain them in simple language.\n",
    "    Format your response in a clear, organized manner.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model = \"internvl\", ##switch to OpenAI model (e.g. gpt-4) for OpenAI implementation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ],\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "        \n",
    "        # Parse the response content\n",
    "        response_summary_dict = parse_llm_summary(response.choices[0].text)\n",
    "\n",
    "        return response_summary_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_llm_summary(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse LLM response to separate thinking process and summary after \n",
    "    summarize_content()\n",
    "    \"\"\"\n",
    "    # Pattern for think tags\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    \n",
    "    # Extract thinking process (everything between think tags)\n",
    "    thinking = re.search(think_pattern, response_text, re.DOTALL)\n",
    "    thinking = thinking.group(1).strip() if thinking else \"\"\n",
    "    \n",
    "    # Get summary (everything after </think>)\n",
    "    summary = re.split(r'</think>\\s*', response_text)[-1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"thinking\": thinking,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "\n",
    "def get_reddit_trends(reddit: praw.Reddit, llm: OpenAI, limit: int = 10) -> List[Dict]:\n",
    "    \"\"\"Fetch and analyze Reddit trends\"\"\"\n",
    "    trends = []\n",
    "    print(f\"üéØ ACTION: Fetching {limit} most popular threads:\")\n",
    "    print(\"=\" * 50)\n",
    "    for submission in reddit.subreddit('technology').hot(limit=limit):\n",
    "        content = extract_article_content(submission.url) or submission.selftext\n",
    "        print(submission.title)\n",
    "        relevance = analyze_genai_relevance(llm, submission.title)\n",
    "        print(f\"GenAI Relevance: {relevance['response']['is_genai_related']}\")\n",
    "        if relevance['response']['is_genai_related']:\n",
    "            print(f\"üéØ ACTION: üìñ Reading Article Details at {submission.url}\")\n",
    "            llm_summary = summarize_content(llm, content) if content else None\n",
    "            trends.append({\n",
    "                'title': submission.title,\n",
    "                'subreddit': submission.subreddit.display_name,\n",
    "                'score': submission.score,\n",
    "                'comments': submission.num_comments,\n",
    "                'url': submission.url,\n",
    "                'relevance': relevance,\n",
    "                'summary': llm_summary['summary']\n",
    "            })\n",
    "        print(\"=\" * 50)\n",
    "        print(trends)\n",
    "    return trends\n",
    "\n",
    "\n",
    "def create_email_summary(trends_list: list, llm: OpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Create an email-style summary from a structured trends dictionary\n",
    "    \"\"\"\n",
    "    # First, let's format the trends data into a more digestible format for the model\n",
    "    formatted_input = \"Recent AI Trends Analysis:\\n\\n\"\n",
    "    for trend in trends_list:\n",
    "        formatted_input += f\"Title: {trend['title']}\\n\"\n",
    "        formatted_input += f\"Engagement: {trend['score']} points, {trend['comments']} comments\\n\"\n",
    "        formatted_input += f\"Summary: {trend['summary']}\\n\\n\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an AI analyst creating clear, professional  summaries of AI news and trends. \n",
    "    Analyze the provided structured data about AI trends and create a well-organized summary that covers:\n",
    "\n",
    "    1. Main Technologies Discussed\n",
    "    - Extract and categorize key AI technologies mentioned across all trends\n",
    "    - Focus on technical implementations and capabilities\n",
    "\n",
    "    2. Key Trends\n",
    "    - Synthesize patterns across all articles\n",
    "    - Identify emerging themes and industry movements\n",
    "    - Include relevant metrics and engagement data\n",
    "\n",
    "    3. Public Sentiment\n",
    "    - Analyze reactions based on comments and scoring\n",
    "    - Note any controversial or highly-engaged topics\n",
    "    - Identify areas of public concern or interest\n",
    "\n",
    "    4. Notable Developments\n",
    "    - Highlight significant announcements or findings\n",
    "    - Include specific numbers, statistics, or metrics\n",
    "    - Note any regulatory or policy changes\n",
    "\n",
    "    Format your response as a professional summary with clear headers and bullet points.\n",
    "    Use engagement metrics (score and comments) to help gauge importance of different topics.\"\"\"\n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model = \"internvl\", ##switch to OpenAI model (e.g. gpt-4) for OpenAI implementation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": formatted_input}\n",
    "            ],\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "        llm_response = response.choices[0].text\n",
    "\n",
    "        # Split Thinking\n",
    "        end_think_pos = llm_response.find('</think>')\n",
    "        thinking_response = llm_response[:end_think_pos]\n",
    "        summary = llm_response[end_think_pos+9:]\n",
    "        f_thinking_response = \"### Deepseek Reasoning\\n\\n\" + thinking_response + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "        \n",
    "        # Add Further Reading section\n",
    "        further_reading = \"\\n\\n---\\n\\n### Further Reading\\n\\n\"\n",
    "        for trend in trends_list:\n",
    "            further_reading += f\"**{trend['title']}**\\n\"\n",
    "            further_reading += f\"- Source: {trend['url']}\\n\\n\"\n",
    "\n",
    "        # Combine AI analysis with Further Reading\n",
    "        complete_email = f_thinking_response + summary + further_reading\n",
    "        \n",
    "        return display(Markdown(complete_email))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in trends summarization: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Agent (the Orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditAIAnalysisAgent:\n",
    "    def __init__(self, reddit_creds: dict, openai_api_key: str):\n",
    "        self.reddit_creds = reddit_creds\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.reddit = None\n",
    "        self.llm = None\n",
    "        self.thought_history = []\n",
    "        print(\"\\nü§ñ Initializing Reddit AI Analysis Agent...\\n\")\n",
    "        \n",
    "    def think(self, thought: str):\n",
    "        \"\"\"Record agent's thinking process\"\"\"\n",
    "        self.thought_history.append({\"thought\": thought, \"timestamp\": datetime.now().isoformat()})\n",
    "        print(f\"\\nü§î THINKING: {thought}\")\n",
    "        \n",
    "    def act(self, action: str, result: any):\n",
    "        \"\"\"Record agent's actions and results\"\"\"\n",
    "        self.thought_history.append({\n",
    "            \"action\": action,\n",
    "            \"result\": result,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        print(f\"üéØ ACTION: {action}\")\n",
    "        print(f\"üìù RESULT: {result}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    def initialize_clients(self) -> bool:\n",
    "        \"\"\"Initialize Reddit and LLM clients\"\"\"\n",
    "        try:\n",
    "            print(\"\\nüì° INITIALIZING CLIENTS...\")\n",
    "            self.think(\"Need to initialize Reddit and LLM client\")\n",
    "            \n",
    "            self.reddit = init_reddit(\n",
    "                self.reddit_creds['client_id'],\n",
    "                self.reddit_creds['client_secret'],\n",
    "                self.reddit_creds['user_agent']\n",
    "            )\n",
    "            self.act(\"Initialize Reddit client\", \"‚úÖ Reddit client initialized successfully\")\n",
    "            \n",
    "            self.llm = init_llm(self.openai_api_key)\n",
    "            self.act(\"Initialize LLM client\", \"‚úÖ LLM client initialized successfully. DeepSeek-R1-Distill-Llama-8B running on 4xL4 Machine\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.act(\"Initialize clients\", f\"‚ùå Failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def analyze_trends(self) -> Optional[Dict]:\n",
    "        \"\"\"Get and analyze Reddit trends\"\"\"\n",
    "        try:\n",
    "            print(\"\\nüîç ANALYZING REDDIT TRENDS...\")\n",
    "            self.think(\"Fetching Reddit trends for analysis\")\n",
    "            \n",
    "            # Get trends\n",
    "            print(\"\\nüìä Fetching posts from r/technology...\")\n",
    "            trends = get_reddit_trends(self.reddit, self.llm)\n",
    "            \n",
    "            if not trends:\n",
    "                self.think(\"No GenAI trends found in current batch\")\n",
    "                self.act(\"Analyze trends\", \"‚ö†Ô∏è No relevant trends found\")\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"analysis\": \"No GenAI trends found.\",\n",
    "                    \"trends\": [],\n",
    "                    \"count\": 0\n",
    "                }\n",
    "            \n",
    "            # Log initial processing\n",
    "            print(f\"‚úÖ Summarization complete for {len(trends)} trends\")\n",
    "            \n",
    "            self.think(f\"Creating high level email summary for overall GenAI trends found\")\n",
    "            analysis = create_email_summary(trends, self.llm)\n",
    "            \n",
    "            # Log completion without printing details\n",
    "            self.act(\"Create analysis\", f\"‚úÖ Analysis complete for {len(trends)} trends\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"analysis\": analysis,\n",
    "                \"trends\": trends,\n",
    "                \"count\": len(trends),\n",
    "                \"thought_process\": self.thought_history\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.act(\"Analyze trends\", f\"‚ùå Failed: {str(e)}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"thought_process\": self.thought_history\n",
    "            }\n",
    "\n",
    "    def run(self) -> Dict:\n",
    "        \"\"\"Main execution flow with ReAct framework\"\"\"\n",
    "        print(\"\\nüöÄ STARTING REDDIT AI TREND ANALYSIS\\n\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        self.think(\"Starting Reddit AI trend analysis\")\n",
    "        \n",
    "        # Initialize clients\n",
    "        if not self.initialize_clients():\n",
    "            print(\"\\n‚ùå Failed to initialize clients. Aborting...\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"Failed to initialize clients\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"thought_process\": self.thought_history\n",
    "            }\n",
    "        \n",
    "        # Analyze trends\n",
    "        result = self.analyze_trends()\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            self.think(\"Analysis complete, final report generated\")\n",
    "            print(\"\\n‚úÖ ANALYSIS COMPLETE\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"\\nFinal report has been generated in the response.\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Analysis failed. Check error details.\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "def main(reddit_creds: dict, openai_api_key: str) -> dict:\n",
    "    \"\"\"Main function using ReAct agent\"\"\"\n",
    "    agent = RedditAIAnalysisAgent(reddit_creds, openai_api_key)\n",
    "    return agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demo Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Initializing Reddit AI Analysis Agent...\n",
      "\n",
      "\n",
      "üöÄ STARTING REDDIT AI TREND ANALYSIS\n",
      "\n",
      "==================================================\n",
      "\n",
      "ü§î THINKING: Starting Reddit AI trend analysis\n",
      "\n",
      "üì° INITIALIZING CLIENTS...\n",
      "\n",
      "ü§î THINKING: Need to initialize Reddit and LLM client\n",
      "üéØ ACTION: Initialize Reddit client\n",
      "üìù RESULT: ‚úÖ Reddit client initialized successfully\n",
      "\n",
      "==================================================\n",
      "üéØ ACTION: Initialize LLM client\n",
      "üìù RESULT: ‚úÖ LLM client initialized successfully. DeepSeek-R1-Distill-Llama-8B running on 4xL4 Machine\n",
      "\n",
      "==================================================\n",
      "\n",
      "üîç ANALYZING REDDIT TRENDS...\n",
      "\n",
      "ü§î THINKING: Fetching Reddit trends for analysis\n",
      "\n",
      "üìä Fetching posts from r/technology...\n",
      "üéØ ACTION: Fetching 10 most popular threads:\n",
      "==================================================\n",
      "As the Trump admin deletes online data, scientists and digital librarians rush to save it\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[]\n",
      "Workers at NASA Told to ‚ÄòDrop Everything‚Äô to Scrub Mentions of Indigenous People, Women from Its Websites | \"This is a drop everything and reprioritize your day request,\" a directive \"per NASA HQ direction\" stated.\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[]\n",
      "Federal Workers Sue to Disconnect DOGE Server\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[]\n",
      "TikTok‚Äôs algorithm exhibited pro-Republican bias during 2024 presidential race, study finds | Trump videos were more likely to reach Democrats on TikTok than Harris videos were to reach Republicans\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[]\n",
      "Trump orders USDA to take down websites referencing climate crisis\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[]\n",
      "$42B broadband grant program may scrap Biden admin‚Äôs preference for fiber | NTIA nominee to rework Broadband Equity, Access, and Deployment program.\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[]\n",
      "California bill would make AI companies remind kids that chatbots aren‚Äôt people\n",
      "GenAI Relevance: True\n",
      "üéØ ACTION: üìñ Reading Article Details at https://www.theverge.com/news/605728/california-chatbot-bill-child-safety\n",
      "==================================================\n",
      "[{'title': 'California bill would make AI companies remind kids that chatbots aren‚Äôt people', 'subreddit': 'technology', 'score': 1216, 'comments': 50, 'url': 'https://www.theverge.com/news/605728/california-chatbot-bill-child-safety', 'relevance': {'thinking': \"Okay, so I'm trying to figure out if the article about a California bill that would make AI companies remind kids that chatbots aren't people is related to Generative AI. Let me break this down step by step.\\n\\nFirst, I know that Generative AI refers to AI systems that can create or generate content, like text, images, or music. Examples include chatbots, language models, and other tools that can produce creative content. So, the article is talking about a bill in California that wants AI companies to inform kids that chatbots aren't real people. That sounds like it's about AI's interaction with users, specifically children.\\n\\nThe relevance here is that it's about how AI is being used in a way that affects users, especially minors. The bill is aiming to set guidelines or regulations for AI interactions, which could influence how companies design their chatbots and ensure they don't mislead users, especially children, into thinking they're talking to a real person. This is more about the ethical and regulatory aspects of AI rather than the generation of content itself.\\n\\nSo, while the article is about AI, it's not directly about generating content but more about the legal and ethical considerations surrounding AI interactions. Therefore, the relevance type would be indirect because it's about the societal impact and regulations of AI rather than the AI's generative capabilities.\", 'response': {'is_genai_related': True, 'relevance_type': 'indirect'}}, 'summary': '```json\\n{\\n    \"is_genai_related\": true,\\n    \"relevance_type\": \"indirect\"\\n}\\n```'}]\n",
      "Google Lifts a Ban on Using Its AI for Weapons and Surveillance\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[{'title': 'California bill would make AI companies remind kids that chatbots aren‚Äôt people', 'subreddit': 'technology', 'score': 1216, 'comments': 50, 'url': 'https://www.theverge.com/news/605728/california-chatbot-bill-child-safety', 'relevance': {'thinking': \"Okay, so I'm trying to figure out if the article about a California bill that would make AI companies remind kids that chatbots aren't people is related to Generative AI. Let me break this down step by step.\\n\\nFirst, I know that Generative AI refers to AI systems that can create or generate content, like text, images, or music. Examples include chatbots, language models, and other tools that can produce creative content. So, the article is talking about a bill in California that wants AI companies to inform kids that chatbots aren't real people. That sounds like it's about AI's interaction with users, specifically children.\\n\\nThe relevance here is that it's about how AI is being used in a way that affects users, especially minors. The bill is aiming to set guidelines or regulations for AI interactions, which could influence how companies design their chatbots and ensure they don't mislead users, especially children, into thinking they're talking to a real person. This is more about the ethical and regulatory aspects of AI rather than the generation of content itself.\\n\\nSo, while the article is about AI, it's not directly about generating content but more about the legal and ethical considerations surrounding AI interactions. Therefore, the relevance type would be indirect because it's about the societal impact and regulations of AI rather than the AI's generative capabilities.\", 'response': {'is_genai_related': True, 'relevance_type': 'indirect'}}, 'summary': '```json\\n{\\n    \"is_genai_related\": true,\\n    \"relevance_type\": \"indirect\"\\n}\\n```'}]\n",
      "A Coup Is In Progress In America\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[{'title': 'California bill would make AI companies remind kids that chatbots aren‚Äôt people', 'subreddit': 'technology', 'score': 1216, 'comments': 50, 'url': 'https://www.theverge.com/news/605728/california-chatbot-bill-child-safety', 'relevance': {'thinking': \"Okay, so I'm trying to figure out if the article about a California bill that would make AI companies remind kids that chatbots aren't people is related to Generative AI. Let me break this down step by step.\\n\\nFirst, I know that Generative AI refers to AI systems that can create or generate content, like text, images, or music. Examples include chatbots, language models, and other tools that can produce creative content. So, the article is talking about a bill in California that wants AI companies to inform kids that chatbots aren't real people. That sounds like it's about AI's interaction with users, specifically children.\\n\\nThe relevance here is that it's about how AI is being used in a way that affects users, especially minors. The bill is aiming to set guidelines or regulations for AI interactions, which could influence how companies design their chatbots and ensure they don't mislead users, especially children, into thinking they're talking to a real person. This is more about the ethical and regulatory aspects of AI rather than the generation of content itself.\\n\\nSo, while the article is about AI, it's not directly about generating content but more about the legal and ethical considerations surrounding AI interactions. Therefore, the relevance type would be indirect because it's about the societal impact and regulations of AI rather than the AI's generative capabilities.\", 'response': {'is_genai_related': True, 'relevance_type': 'indirect'}}, 'summary': '```json\\n{\\n    \"is_genai_related\": true,\\n    \"relevance_type\": \"indirect\"\\n}\\n```'}]\n",
      "Spotify Adds 35 Million Users, Hits First Full-Year Profit\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "[{'title': 'California bill would make AI companies remind kids that chatbots aren‚Äôt people', 'subreddit': 'technology', 'score': 1216, 'comments': 50, 'url': 'https://www.theverge.com/news/605728/california-chatbot-bill-child-safety', 'relevance': {'thinking': \"Okay, so I'm trying to figure out if the article about a California bill that would make AI companies remind kids that chatbots aren't people is related to Generative AI. Let me break this down step by step.\\n\\nFirst, I know that Generative AI refers to AI systems that can create or generate content, like text, images, or music. Examples include chatbots, language models, and other tools that can produce creative content. So, the article is talking about a bill in California that wants AI companies to inform kids that chatbots aren't real people. That sounds like it's about AI's interaction with users, specifically children.\\n\\nThe relevance here is that it's about how AI is being used in a way that affects users, especially minors. The bill is aiming to set guidelines or regulations for AI interactions, which could influence how companies design their chatbots and ensure they don't mislead users, especially children, into thinking they're talking to a real person. This is more about the ethical and regulatory aspects of AI rather than the generation of content itself.\\n\\nSo, while the article is about AI, it's not directly about generating content but more about the legal and ethical considerations surrounding AI interactions. Therefore, the relevance type would be indirect because it's about the societal impact and regulations of AI rather than the AI's generative capabilities.\", 'response': {'is_genai_related': True, 'relevance_type': 'indirect'}}, 'summary': '```json\\n{\\n    \"is_genai_related\": true,\\n    \"relevance_type\": \"indirect\"\\n}\\n```'}]\n",
      "‚úÖ Summarization complete for 1 trends\n",
      "\n",
      "ü§î THINKING: Creating high level email summary for overall GenAI trends found\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Deepseek Reasoning\n",
       "\n",
       "<think>\n",
       "Okay, so I need to create a summary of recent AI trends based on the provided data. Let me start by understanding what the user is asking for. They want a structured summary that covers four main areas: Main Technologies, Key Trends, Public Sentiment, and Notable Developments. Each section should be well-organized with bullet points and include engagement metrics to gauge importance.\n",
       "\n",
       "First, looking at the data provided, there's only one article about a California bill aimed at making AI companies remind kids that chatbots aren't people. The engagement metrics show 1216 points and 50 comments, which indicates moderate engagement. The relevance type is indirect, meaning it's related to AI but not directly about AI technology.\n",
       "\n",
       "For the Main Technologies section, I need to extract key AI technologies mentioned. The article doesn't specify any particular technologies like NLP or machine learning, so maybe I should note that it's more about policy and regulations rather than specific tech implementations.\n",
       "\n",
       "Next, Key Trends. The main trend here is the push for ethical AI use, specifically regarding transparency and user awareness. The bill is a response to concerns about AI interacting with minors, so the trend is about regulation and ethical practices in AI.\n",
       "\n",
       "Public Sentiment. The comments are mixed, with some supporting the bill and others concerned about its impact on AI development. The sentiment is neutral to slightly positive, but there's a notable concern about over-regulation.\n",
       "\n",
       "Notable Developments. The bill itself is the main development, with specific details like the requirement for disclaimers and the need for clear labeling. There's also a mention of the bill's potential impact on the AI industry, so I should include that.\n",
       "\n",
       "I should structure each section clearly, using headers and bullet points. For engagement metrics, I'll list them under each relevant point to show which topics are more engaging. I'll make sure the summary is concise and professional, avoiding any markdown as per the user's request.\n",
       "\n",
       "Wait, the user mentioned using headers and bullet points, but in the response example, they used headers without markdown. So I'll follow that style, using clear headers and bullet points without any markdown formatting.\n",
       "\n",
       "I think I've covered all the points. Now, I'll organize the information accordingly, ensuring each section flows logically and the key points are highlighted. I'll also make sure to include the engagement metrics to help prioritize the importance of each topic.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "### Summary of Recent AI Trends\n",
       "\n",
       "#### 1. **Main Technologies Discussed**\n",
       "- **Policy and Regulations**: The article focuses on the ethical and legal aspects of AI interactions, particularly with minors.\n",
       "- **Technological Implementation**: While no specific AI technologies are highlighted, the discussion emphasizes the need for transparency and user awareness in AI systems.\n",
       "\n",
       "#### 2. **Key Trends**\n",
       "- **Ethical AI Use**: A growing emphasis on ensuring AI systems are transparent and do not mislead users, especially in interactions with minors.\n",
       "- **Regulatory Response**: Legislative efforts to address AI-related ethical concerns, such as the California bill requiring chatbots to clarify their non-human status.\n",
       "- **Industry Impact**: Potential shifts in AI development and deployment to align with regulatory standards.\n",
       "\n",
       "#### 3. **Public Sentiment**\n",
       "- **Support and Concern**: Public opinion is mixed, with some supporting the bill as a necessary step for user protection and others worried about its broader implications for AI innovation.\n",
       "- **Engagement Metrics**: 1216 points and 50 comments indicate moderate public interest and debate around the topic.\n",
       "- **Concerns**: A notable portion of comments express fear about over-regulation stifling AI progress.\n",
       "\n",
       "#### 4. **Notable Developments**\n",
       "- **Legislative Action**: The introduction of the California bill (AB 1351) mandates AI companies to use disclaimers and clear labeling to distinguish chatbots from humans.\n",
       "- **Significance**: The bill is seen as a potential model for other regions and industries to follow, highlighting the need for ethical AI practices.\n",
       "- **Impact on Industry**: The proposed regulations could influence how AI companies design and market their products, potentially increasing transparency and user trust.\n",
       "\n",
       "This analysis underscores the evolving landscape of AI ethics and the growing role of regulatory frameworks in shaping AI development and public perception.\n",
       "\n",
       "---\n",
       "\n",
       "### Further Reading\n",
       "\n",
       "**California bill would make AI companies remind kids that chatbots aren‚Äôt people**\n",
       "- Source: https://www.theverge.com/news/605728/california-chatbot-bill-child-safety\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ACTION: Create analysis\n",
      "üìù RESULT: ‚úÖ Analysis complete for 1 trends\n",
      "\n",
      "==================================================\n",
      "\n",
      "ü§î THINKING: Analysis complete, final report generated\n",
      "\n",
      "‚úÖ ANALYSIS COMPLETE\n",
      "==================================================\n",
      "\n",
      "Final report has been generated in the response.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file in current directory\n",
    "load_dotenv()\n",
    "\n",
    "reddit_creds = {\n",
    "    \"client_id\": os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    \"client_secret\": os.getenv(\"REDDIT_CLIENT_SECRET\"), \n",
    "    \"user_agent\": os.getenv(\"REDDIT_USER_AGENT\")\n",
    "}\n",
    "\n",
    "openai_api_key = \"no api needed\" ##switch to openAI key when for OpenAI implementation\n",
    "\n",
    "result = main(reddit_creds, openai_api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
