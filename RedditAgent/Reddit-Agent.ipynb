{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit GenAI Trend Analysis with ReAct Agent Framework\n",
    "\n",
    "Author: Amanda Milberg, Principal Solutions Engineer @ TitanML\n",
    "\n",
    "üéØ **Main Purpose**:\n",
    "- Analyzes r/technology subreddit posts to identify and summarize GenAI-related content\n",
    "- Generates professional summaries of AI trends and developments to send to downstream users who want to stay up to date on the latest trends\n",
    "\n",
    "üîë **Key Components**:\n",
    "1. Reddit API Integration to scrape relevant posts in a given subreddit (e.g. r/technology)\n",
    "2. LLM-powered analysis to:\n",
    "   - Determine GenAI relevance based on the thread title\n",
    "   - Summarize key themes and content for each article\n",
    "   - Generate trend analysis summary reports for all the GenAI related articles \n",
    "\n",
    "üìä **Process Flow**:\n",
    "1. Fetches hot posts from r/technology \n",
    "2. Filters for GenAI-related content\n",
    "3. Extracts and summarizes article content\n",
    "4. Creates comprehensive trend analysis\n",
    "5. Generates formatted report with sources ready to email to downstream users \n",
    "\n",
    "üõ†Ô∏è **Technologies Used**:\n",
    "- PRAW (Reddit API)\n",
    "- OpenAI API/Self-hosted LLM\n",
    "- BeautifulSoup for web scraping\n",
    "- Markdown for report formatting\n",
    "- ReAct agent framework\n",
    "\n",
    "_Note: Requires Reddit API credentials and access to a LLM to function._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use an Agent Framework?\n",
    "\n",
    "- Implements the ReAct (Reasoning + Acting) paradigm for more transparent and controlled AI behavior\n",
    "- Provides explicit thinking and action steps for complex tasks\n",
    "- Enables better debugging and monitoring of the AI's decision process\n",
    "\n",
    "üß† **ReAct Framework Benefits**:\n",
    "1. **Reasoning Transparency**\n",
    "   - Agent explicitly shows its thinking process before actions\n",
    "   - Helps track decision-making logic\n",
    "   - Makes debugging easier\n",
    "\n",
    "2. **Structured Actions**\n",
    "   - Clear separation between thinking and execution\n",
    "   - Each action has defined inputs and outputs\n",
    "   - Better error handling and recovery\n",
    "\n",
    "3. **Process Monitoring**\n",
    "   - Logs each step of the analysis pipeline\n",
    "   - Tracks success/failure of individual components\n",
    "   - Maintains history of decisions and actions\n",
    "\n",
    "_The agent framework transforms what could be a simple script into a more robust, observable, and maintainable system for AI analysis. The agent approach provides better structure, transparency, and reliability for complex AI tasks compared to a simple main function._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Self-Host?\n",
    "\n",
    "üåü **Key Benefits of Self-Hosting** \n",
    "\n",
    "1. **Cost-Effective Performance**\n",
    "   - Reduced operational costs for high-volume processing\n",
    "   - No ongoing API fees or usage limits\n",
    "\n",
    "2. **Privacy & Data Control** \n",
    "   - Complete control over data processing and storage\n",
    "   - No data sharing with external providers\n",
    "   - Compliance with internal security policies\n",
    "   - Ability to air-gap for sensitive applications & sensitive data \n",
    "\n",
    "3. **Deployment Flexibility**\n",
    "   - Run locally on your own infrastructure\n",
    "   - Scale resources based on actual needs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Deep Seek?\n",
    "\n",
    "1. **Specialized Reasoning Capabilities**\n",
    "   - Optimized for logical reasoning and analysis tasks\n",
    "   - Efficient chain-of-thought processing\n",
    "   - Ideal for structured analytical workflows\n",
    "2. **Open Source Technology + Self-Hosting Stack = üòç**  \n",
    "   - Deepseek broke the internet \n",
    "   - Firm believer in owning your AI stack \n",
    "   - Smaller / specalized models for a given application  \n",
    "\n",
    "_Note: In this demo we are running a self-hosted [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) deployed on 4xL4 GPUs using the [TitanML's Takeoff Stack](https://docs.titanml.co/). If you want to try this on your own you can pull this repository and swap in an OpenAI model. The code uses OpenAI compatiable endpoints so any model should be able to be swapped in. If you have any questions please reach out to: amanda.milberg@titanml.co_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions in AI Agent Architecture (or the \"Doing\")\n",
    "\n",
    "üîß **Service Functions**\n",
    "Functions that handle specific, specialized tasks like:\n",
    "- API interactions (init_reddit, init_llm)\n",
    "- Web scraping (extract_article_content)\n",
    "- Data parsing & formatting (parse_llm_response)\n",
    "- LLM analysis (analyze_genai_relevance, summarize_content, create_email_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "import re\n",
    "import requests\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "\n",
    "def init_reddit(client_id: str, client_secret: str, user_agent: str) -> praw.Reddit:\n",
    "    \"\"\"Initialize Reddit API client\"\"\"\n",
    "    return praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "\n",
    "def init_llm(api_key: str) -> OpenAI:\n",
    "    ## For practice at home, you can sub the self-hosted LLM for openAI LLM\n",
    "    \"\"\"Initialize OpenAI LLM Note: Need access to OpenAI Key\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    client = OpenAI(temperature=0.7)\n",
    "    \"\"\"\n",
    "    ## In our demo we will use a self-hosted LLM \n",
    "    client = OpenAI(\n",
    "    base_url=\"http://rag-demo:3003/v1\",\n",
    "    api_key=\"not needed\"\n",
    "    )\n",
    "\n",
    "    return client\n",
    "\n",
    "\n",
    "def extract_article_content(url: str) -> str:\n",
    "    \"\"\"Extract main content from article URL with proper headers\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return ' '.join(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def analyze_genai_relevance(llm: OpenAI, title: str) -> dict:\n",
    "    \"\"\"Analyze if title is GenAI-related using LLM\"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Based on the title \n",
    "    of the article provide a suggestion if this content relates to Generative AI:\n",
    "    \n",
    "    Return JSON:\n",
    "        {{\n",
    "            \"is_genai_related\": true/false,\n",
    "            \"relevance_type\": \"direct/indirect/none\",\n",
    "        }}\"\"\"    \n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model = \"internvl\", ##switch to OpenAI model (e.g. gpt-4) for OpenAI implementation \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": title}\n",
    "            ],\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "        \n",
    "        # Extract the response content\n",
    "        response_dict = parse_llm_response(response.choices[0].text)\n",
    "        return response_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in GenAI relevance: {str(e)}\")\n",
    "        return \"\"\n",
    "    \n",
    "def parse_llm_response(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse LLM response to separate thinking process and JSON response from \n",
    "    analyze_genai_relevance()\n",
    "    \"\"\"\n",
    "    # Pattern for think tags\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    \n",
    "    # Pattern for JSON (anything between triple backticks and json)\n",
    "    json_pattern = r'```json\\n(.*?)```'\n",
    "    \n",
    "    # Extract thinking process\n",
    "    thinking = re.search(think_pattern, response_text, re.DOTALL)\n",
    "    thinking = thinking.group(1).strip() if thinking else \"\"\n",
    "    \n",
    "    # Extract JSON response\n",
    "    json_match = re.search(json_pattern, response_text, re.DOTALL)\n",
    "    json_str = json_match.group(1).strip() if json_match else \"{}\"\n",
    "    json_data = json.loads(json_str)\n",
    "    \n",
    "    return {\n",
    "        \"thinking\": thinking,\n",
    "        \"response\": json_data\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize_content(llm: OpenAI, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Summarize input text using the chat completions model directly\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Given a piece of text, analyze its content and provide a concise summary.\n",
    "    Focus on extracting key information and main ideas.\n",
    "    If the text contains technical terms, explain them in simple language.\n",
    "    Format your response in a clear, organized manner.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model = \"internvl\", ##switch to OpenAI model (e.g. gpt-4) for OpenAI implementation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ],\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "        \n",
    "        # Parse the response content\n",
    "        response_summary_dict = parse_llm_summary(response.choices[0].text)\n",
    "\n",
    "        return response_summary_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in summarization: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def parse_llm_summary(response_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse LLM response to separate thinking process and summary after \n",
    "    summarize_content()\n",
    "    \"\"\"\n",
    "    # Pattern for think tags\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    \n",
    "    # Extract thinking process (everything between think tags)\n",
    "    thinking = re.search(think_pattern, response_text, re.DOTALL)\n",
    "    thinking = thinking.group(1).strip() if thinking else \"\"\n",
    "    \n",
    "    # Get summary (everything after </think>)\n",
    "    summary = re.split(r'</think>\\s*', response_text)[-1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"thinking\": thinking,\n",
    "        \"summary\": summary\n",
    "    }\n",
    "\n",
    "\n",
    "def get_reddit_trends(reddit: praw.Reddit, llm: OpenAI, limit: int = 10) -> List[Dict]:\n",
    "    \"\"\"Fetch and analyze Reddit trends\"\"\"\n",
    "    trends = []\n",
    "    print(f\"üéØ ACTION: Fetching {limit} most popular threads:\")\n",
    "    print(\"=\" * 50)\n",
    "    for submission in reddit.subreddit('technology').hot(limit=limit):\n",
    "        content = extract_article_content(submission.url) or submission.selftext\n",
    "        print(submission.title)\n",
    "        relevance = analyze_genai_relevance(llm, submission.title)\n",
    "        print(f\"GenAI Relevance: {relevance['response']['is_genai_related']}\")\n",
    "        if relevance['response']['is_genai_related']:\n",
    "            print(f\"üéØ ACTION: üìñ Reading Article Details at {submission.url}\")\n",
    "            llm_summary = summarize_content(llm, content) if content else None\n",
    "            trends.append({\n",
    "                'title': submission.title,\n",
    "                'subreddit': submission.subreddit.display_name,\n",
    "                'score': submission.score,\n",
    "                'comments': submission.num_comments,\n",
    "                'url': submission.url,\n",
    "                'relevance': relevance,\n",
    "                'summary': llm_summary['summary']\n",
    "            })\n",
    "        print(\"=\" * 50)\n",
    "    return trends\n",
    "\n",
    "\n",
    "def create_email_summary(trends_list: list, llm: OpenAI) -> str:\n",
    "    \"\"\"\n",
    "    Create an email-style summary from a structured trends dictionary\n",
    "    \"\"\"\n",
    "    # First, let's format the trends data into a more digestible format for the model\n",
    "    formatted_input = \"Recent AI Trends Analysis:\\n\\n\"\n",
    "    for trend in trends_list:\n",
    "        formatted_input += f\"Title: {trend['title']}\\n\"\n",
    "        formatted_input += f\"Engagement: {trend['score']} points, {trend['comments']} comments\\n\"\n",
    "        formatted_input += f\"Summary: {trend['summary']}\\n\\n\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an AI analyst creating clear, professional  summaries of AI news and trends. \n",
    "    Analyze the provided structured data about AI trends and create a well-organized summary that covers:\n",
    "\n",
    "    1. Main Technologies Discussed\n",
    "    - Extract and categorize key AI technologies mentioned across all trends\n",
    "    - Focus on technical implementations and capabilities\n",
    "\n",
    "    2. Key Trends\n",
    "    - Synthesize patterns across all articles\n",
    "    - Identify emerging themes and industry movements\n",
    "    - Include relevant metrics and engagement data\n",
    "\n",
    "    3. Public Sentiment\n",
    "    - Analyze reactions based on comments and scoring\n",
    "    - Note any controversial or highly-engaged topics\n",
    "    - Identify areas of public concern or interest\n",
    "\n",
    "    4. Notable Developments\n",
    "    - Highlight significant announcements or findings\n",
    "    - Include specific numbers, statistics, or metrics\n",
    "    - Note any regulatory or policy changes\n",
    "\n",
    "    Format your response as a professional summary with clear headers and bullet points.\n",
    "    Use engagement metrics (score and comments) to help gauge importance of different topics.\"\"\"\n",
    "    try:\n",
    "        response = llm.chat.completions.create(\n",
    "            model = \"internvl\", ##switch to OpenAI model (e.g. gpt-4) for OpenAI implementation\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": formatted_input}\n",
    "            ],\n",
    "            max_tokens = 2000\n",
    "        )\n",
    "        llm_response = response.choices[0].text\n",
    "\n",
    "        # Split Thinking\n",
    "        end_think_pos = llm_response.find('</think>')\n",
    "        thinking_response = llm_response[:end_think_pos]\n",
    "        summary = llm_response[end_think_pos+9:]\n",
    "        f_thinking_response = \"### Deepseek Reasoning\\n\\n\" + thinking_response + \"\\n\\n---\\n\\n\"\n",
    "\n",
    "        \n",
    "        # Add Further Reading section\n",
    "        further_reading = \"\\n\\n---\\n\\n### Further Reading\\n\\n\"\n",
    "        for trend in trends_list:\n",
    "            further_reading += f\"**{trend['title']}**\\n\"\n",
    "            further_reading += f\"- Source: {trend['url']}\\n\\n\"\n",
    "\n",
    "        # Combine AI analysis with Further Reading\n",
    "        complete_email = f_thinking_response + summary + further_reading\n",
    "        \n",
    "        return display(Markdown(complete_email))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in trends summarization: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Agent (the Orchestrator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditAIAnalysisAgent:\n",
    "    def __init__(self, reddit_creds: dict, openai_api_key: str):\n",
    "        self.reddit_creds = reddit_creds\n",
    "        self.openai_api_key = openai_api_key\n",
    "        self.reddit = None\n",
    "        self.llm = None\n",
    "        self.thought_history = []\n",
    "        print(\"\\nü§ñ Initializing Reddit AI Analysis Agent...\\n\")\n",
    "        \n",
    "    def think(self, thought: str):\n",
    "        \"\"\"Record agent's thinking process\"\"\"\n",
    "        self.thought_history.append({\"thought\": thought, \"timestamp\": datetime.now().isoformat()})\n",
    "        print(f\"\\nü§î THINKING: {thought}\")\n",
    "        \n",
    "    def act(self, action: str, result: any):\n",
    "        \"\"\"Record agent's actions and results\"\"\"\n",
    "        self.thought_history.append({\n",
    "            \"action\": action,\n",
    "            \"result\": result,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        print(f\"üéØ ACTION: {action}\")\n",
    "        print(f\"üìù RESULT: {result}\\n\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    def initialize_clients(self) -> bool:\n",
    "        \"\"\"Initialize Reddit and LLM clients\"\"\"\n",
    "        try:\n",
    "            print(\"\\nüì° INITIALIZING CLIENTS...\")\n",
    "            self.think(\"Need to initialize Reddit and LLM client\")\n",
    "            \n",
    "            self.reddit = init_reddit(\n",
    "                self.reddit_creds['client_id'],\n",
    "                self.reddit_creds['client_secret'],\n",
    "                self.reddit_creds['user_agent']\n",
    "            )\n",
    "            self.act(\"Initialize Reddit client\", \"‚úÖ Reddit client initialized successfully\")\n",
    "            \n",
    "            self.llm = init_llm(self.openai_api_key)\n",
    "            self.act(\"Initialize LLM client\", \"‚úÖ LLM client initialized successfully. DeepSeek-R1-Distill-Llama-8B running on 4xL4 Machine\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.act(\"Initialize clients\", f\"‚ùå Failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def analyze_trends(self) -> Optional[Dict]:\n",
    "        \"\"\"Get and analyze Reddit trends\"\"\"\n",
    "        try:\n",
    "            print(\"\\nüîç ANALYZING REDDIT TRENDS...\")\n",
    "            self.think(\"Fetching Reddit trends for analysis\")\n",
    "            \n",
    "            # Get trends\n",
    "            print(\"\\nüìä Fetching posts from r/technology...\")\n",
    "            trends = get_reddit_trends(self.reddit, self.llm)\n",
    "            \n",
    "            if not trends:\n",
    "                self.think(\"No GenAI trends found in current batch\")\n",
    "                self.act(\"Analyze trends\", \"‚ö†Ô∏è No relevant trends found\")\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"analysis\": \"No GenAI trends found.\",\n",
    "                    \"trends\": [],\n",
    "                    \"count\": 0\n",
    "                }\n",
    "            \n",
    "            # Log initial processing\n",
    "            print(f\"‚úÖ Summarization complete for {len(trends)} trends\")\n",
    "            \n",
    "            self.think(f\"Creating high level email summary for overall GenAI trends found\")\n",
    "            analysis = create_email_summary(trends, self.llm)\n",
    "            \n",
    "            # Log completion without printing details\n",
    "            self.act(\"Create analysis\", f\"‚úÖ Analysis complete for {len(trends)} trends\")\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"analysis\": analysis,\n",
    "                \"trends\": trends,\n",
    "                \"count\": len(trends),\n",
    "                \"thought_process\": self.thought_history\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.act(\"Analyze trends\", f\"‚ùå Failed: {str(e)}\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"thought_process\": self.thought_history\n",
    "            }\n",
    "\n",
    "    def run(self) -> Dict:\n",
    "        \"\"\"Main execution flow with ReAct framework\"\"\"\n",
    "        print(\"\\nüöÄ STARTING REDDIT AI TREND ANALYSIS\\n\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        self.think(\"Starting Reddit AI trend analysis\")\n",
    "        \n",
    "        # Initialize clients\n",
    "        if not self.initialize_clients():\n",
    "            print(\"\\n‚ùå Failed to initialize clients. Aborting...\")\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": \"Failed to initialize clients\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"thought_process\": self.thought_history\n",
    "            }\n",
    "        \n",
    "        # Analyze trends\n",
    "        result = self.analyze_trends()\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            self.think(\"Analysis complete, final report generated\")\n",
    "            print(\"\\n‚úÖ ANALYSIS COMPLETE\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"\\nFinal report has been generated in the response.\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Analysis failed. Check error details.\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "def main(reddit_creds: dict, openai_api_key: str) -> dict:\n",
    "    \"\"\"Main function using ReAct agent\"\"\"\n",
    "    agent = RedditAIAnalysisAgent(reddit_creds, openai_api_key)\n",
    "    return agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Demo Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Initializing Reddit AI Analysis Agent...\n",
      "\n",
      "\n",
      "üöÄ STARTING REDDIT AI TREND ANALYSIS\n",
      "\n",
      "==================================================\n",
      "\n",
      "ü§î THINKING: Starting Reddit AI trend analysis\n",
      "\n",
      "üì° INITIALIZING CLIENTS...\n",
      "\n",
      "ü§î THINKING: Need to initialize Reddit and LLM client\n",
      "üéØ ACTION: Initialize Reddit client\n",
      "üìù RESULT: ‚úÖ Reddit client initialized successfully\n",
      "\n",
      "==================================================\n",
      "üéØ ACTION: Initialize LLM client\n",
      "üìù RESULT: ‚úÖ LLM client initialized successfully. DeepSeek-R1-Distill-Llama-8B running on 4xL4 Machine\n",
      "\n",
      "==================================================\n",
      "\n",
      "üîç ANALYZING REDDIT TRENDS...\n",
      "\n",
      "ü§î THINKING: Fetching Reddit trends for analysis\n",
      "\n",
      "üìä Fetching posts from r/technology...\n",
      "üéØ ACTION: Fetching 10 most popular threads:\n",
      "==================================================\n",
      "As the Trump admin deletes online data, scientists and digital librarians rush to save it\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "Workers at NASA Told to ‚ÄòDrop Everything‚Äô to Scrub Mentions of Indigenous People, Women from Its Websites | \"This is a drop everything and reprioritize your day request,\" a directive \"per NASA HQ direction\" stated.\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "Federal Workers Sue to Disconnect DOGE Server\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "TikTok‚Äôs algorithm exhibited pro-Republican bias during 2024 presidential race, study finds | Trump videos were more likely to reach Democrats on TikTok than Harris videos were to reach Republicans\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "Trump orders USDA to take down websites referencing climate crisis\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "$42B broadband grant program may scrap Biden admin‚Äôs preference for fiber | NTIA nominee to rework Broadband Equity, Access, and Deployment program.\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "California bill would make AI companies remind kids that chatbots aren‚Äôt people\n",
      "GenAI Relevance: True\n",
      "üéØ ACTION: üìñ Reading Article Details at https://www.theverge.com/news/605728/california-chatbot-bill-child-safety\n",
      "==================================================\n",
      "Google Lifts a Ban on Using Its AI for Weapons and Surveillance\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "Google removes pledge to not use AI for weapons from website\n",
      "GenAI Relevance: True\n",
      "üéØ ACTION: üìñ Reading Article Details at https://techcrunch.com/2025/02/04/google-removes-pledge-to-not-use-ai-for-weapons-from-website/\n",
      "==================================================\n",
      "A Coup Is In Progress In America\n",
      "GenAI Relevance: False\n",
      "==================================================\n",
      "‚úÖ Summarization complete for 2 trends\n",
      "\n",
      "ü§î THINKING: Creating high level email summary for overall GenAI trends found\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Deepseek Reasoning\n",
       "\n",
       "<think>\n",
       "Okay, so I need to create a summary of the provided AI news and trends. Let me start by reading through each article carefully.\n",
       "\n",
       "First, the California bill that makes AI companies remind kids that chatbots aren't people. It has 1295 points and 50 comments. That's a decent engagement, but the relevance type is indirect. I should note that as a key point under public sentiment, maybe highlighting the concern around AI's role in misleading users, especially children.\n",
       "\n",
       "Next, Google removing their pledge against using AI for weapons. This has 311 points and 38 comments. It's a significant development. I need to categorize this under main technologies as it involves AI ethics and military applications. The summary mentions the removal of a specific pledge and the updated principles, so I should extract that. Also, the public sentiment here is negative, with concerns about ethics and military use.\n",
       "\n",
       "I should structure the summary with clear headers: Main Technologies, Key Trends, Public Sentiment, and Notable Developments. Under each, I'll list bullet points. For main technologies, I'll include NLP for chatbots and AI ethics. For trends, the main themes are ethical concerns and regulatory issues. Public sentiment will cover both the bill and Google's decision, noting the negative reactions. Notable developments will focus on the bill and Google's change, including the engagement metrics to show their importance.\n",
       "\n",
       "I need to make sure each section is concise and uses bullet points for clarity. Also, I should ensure that the engagement metrics are included where relevant to highlight the importance of each topic. I should avoid any markdown and keep the language professional.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "\n",
       "### Summary of AI Trends Analysis\n",
       "\n",
       "#### 1. **Main Technologies Discussed**\n",
       "- **Natural Language Processing (NLP):** Used in chatbots, highlighting the need for clear distinctions between AI and human interaction, especially for children.\n",
       "- **AI Ethics and Military Applications:** Google's use of AI in military contexts and the ethical implications of such applications.\n",
       "\n",
       "#### 2. **Key Trends**\n",
       "- **Ethical Concerns and Regulatory Issues:** The push for clearer ethical guidelines and regulations in AI development and use.\n",
       "- **Public Awareness and Misleading Interactions:** Concerns about AI's role in misleading users, particularly children, as seen in the California bill.\n",
       "\n",
       "#### 3. **Public Sentiment**\n",
       "- **Negative Sentiment on Google's Decision:** Public concern over Google's involvement in military AI applications, with 311 points and 38 comments.\n",
       "- **Support for Ethical AI Use:** The California bill received 1295 points and 50 comments, indicating strong public support for ethical AI use and protecting users, especially children.\n",
       "\n",
       "#### 4. **Notable Developments**\n",
       "- **California Bill:** A new law requiring AI companies to inform users that chatbots are not people, with significant engagement of 1295 points and 50 comments.\n",
       "- **Google's AI Principles Update:** Removal of the pledge against AI for weapons, leading to 311 points and 38 comments, highlighting ethical and military application concerns.\n",
       "\n",
       "This analysis captures the key technologies, trends, public reactions, and significant developments in the AI landscape, providing a comprehensive overview of current AI-related news.\n",
       "\n",
       "---\n",
       "\n",
       "### Further Reading\n",
       "\n",
       "**California bill would make AI companies remind kids that chatbots aren‚Äôt people**\n",
       "- Source: https://www.theverge.com/news/605728/california-chatbot-bill-child-safety\n",
       "\n",
       "**Google removes pledge to not use AI for weapons from website**\n",
       "- Source: https://techcrunch.com/2025/02/04/google-removes-pledge-to-not-use-ai-for-weapons-from-website/\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ACTION: Create analysis\n",
      "üìù RESULT: ‚úÖ Analysis complete for 2 trends\n",
      "\n",
      "==================================================\n",
      "\n",
      "ü§î THINKING: Analysis complete, final report generated\n",
      "\n",
      "‚úÖ ANALYSIS COMPLETE\n",
      "==================================================\n",
      "\n",
      "Final report has been generated in the response.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file in current directory\n",
    "load_dotenv()\n",
    "\n",
    "reddit_creds = {\n",
    "    \"client_id\": os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    \"client_secret\": os.getenv(\"REDDIT_CLIENT_SECRET\"), \n",
    "    \"user_agent\": os.getenv(\"REDDIT_USER_AGENT\")\n",
    "}\n",
    "\n",
    "openai_api_key = \"no api needed\" ##switch to openAI key when for OpenAI implementation\n",
    "\n",
    "result = main(reddit_creds, openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
